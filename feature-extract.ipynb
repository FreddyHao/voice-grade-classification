{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------\n",
    "### 第一步 语音流程度特征提取\n",
    "\n",
    "通过发音的流程度 进行一个分类\n",
    "\n",
    "安装依赖\n",
    "```\n",
    "pip install librosa\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def MaxMinNormalization(x):\n",
    "    \"\"\"\n",
    "    线性归一化，将输入list归一化\n",
    "    :param x: list类型\n",
    "    :return: 归一化list\n",
    "    \"\"\"\n",
    "    x = (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "    return x\n",
    "\n",
    "\n",
    "def normalization(list):\n",
    "    \"\"\"\n",
    "    归一化接口，目前只支持线性归一化\n",
    "    :param list: 矩阵形式\n",
    "    :return: 归一化矩阵\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for x in list:\n",
    "        out.append(MaxMinNormalization(x))\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_max(list):\n",
    "    \"\"\"\n",
    "    提取音频序列中的极大值特征\n",
    "    :param list，宽度固定为20维，长度不限\n",
    "    :return:20维数组\n",
    "    \"\"\"\n",
    "    average = []\n",
    "    arr_temp = np.array(list)\n",
    "    # arr_temp=np.dot(arr_temp,arr_temp.T)\n",
    "    for a in arr_temp:\n",
    "        average.append(max(a))\n",
    "    # average.append(math.atan(max(a)) * 2 / 3.1415926)\n",
    "    return average\n",
    "\n",
    "\n",
    "def load(file):\n",
    "    \"\"\"\n",
    "    输入文件名，加载数据\n",
    "    :param file:文件名\n",
    "    :return:浮点型数组\n",
    "    \"\"\"\n",
    "    list = []\n",
    "    f = open(file, 'r', encoding='UTF-8')\n",
    "    for line in f:\n",
    "        line_list = line.replace(',\\n', '').split(',')\n",
    "        for i in range(len(line_list)):\n",
    "            line_list[i] = float(line_list[i])\n",
    "        list.append(line_list)\n",
    "    return list\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    \"\"\"\n",
    "    获取所有数据，包括音频mfcc特征数据和标签数据\n",
    "    x_data:[[20],[20]]\n",
    "    :return: x_data,y_data\n",
    "    \"\"\"\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    src_path = 'dataset/test_chinese/'\n",
    "    filename = os.listdir(src_path)\n",
    "    for item in filename:  # 进入到文件夹内，对每个文件进行循环遍历\n",
    "        y, sr = librosa.load(src_path + item)\n",
    "        a = librosa.feature.mfcc(y=y, sr=sr)\n",
    "        x_data.append(get_max(a))\n",
    "        y_data.append(0)\n",
    "    src_path = 'dataset/test_english/'\n",
    "    filename = os.listdir(src_path)\n",
    "    for item in filename:  # 进入到文件夹内，对每个文件进行循环遍历\n",
    "        y, sr = librosa.load(src_path + item)\n",
    "        a = librosa.feature.mfcc(y=y, sr=sr)\n",
    "        x_data.append(get_max(a))\n",
    "        y_data.append(1)\n",
    "    print(\"OK\")\n",
    "    return x_data, y_data\n",
    "\n",
    "\n",
    "def shuffer(x, y):\n",
    "    \"\"\"\n",
    "    打乱数据\n",
    "    :param x: [[20],[20]]\n",
    "    :param y: [[0,1,0],[1,0,0]]]onehot数据\n",
    "    :return: x_out，y_out打乱的数据\n",
    "    \"\"\"\n",
    "    x_out = []\n",
    "    y_out = []\n",
    "    all = []\n",
    "    for i in range(0, len(y)):\n",
    "        all.append([x[i], y[i]])\n",
    "    import random\n",
    "    random.seed(0)\n",
    "    random.shuffle(all)\n",
    "    for item in all:\n",
    "        x_out.append(item[0])\n",
    "        y_out.append(item[1])\n",
    "    return x_out, y_out\n",
    "\n",
    "\n",
    "def data_split(x, y, rate):\n",
    "    \"\"\"\n",
    "    通过设定训练集和验证集的比率，来调节数据\n",
    "    :param x: 输入矩阵\n",
    "    :param y: 输出矩阵\n",
    "    :param rate: 浮点型0-1之间\n",
    "    :return: train_data，test_data\n",
    "    \"\"\"\n",
    "    num = int(rate * len(y))\n",
    "    train_data = [x[:num], y[:num]]\n",
    "    test_data = [x[num:], y[num:]]\n",
    "    return train_data, test_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_data(input_file_dir):\n",
    "    \"\"\"\n",
    "\n",
    "    :param input_file_dir: 声音文件保存路径\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    label_count = 0\n",
    "    for f in os.listdir(input_file_dir):\n",
    "        print('foler: ', f)\n",
    "        src_path = '{}/{}'.format(input_file_dir, f)\n",
    "        file_list = os.listdir(src_path)\n",
    "\n",
    "        for item in file_list:  # 进入到文件夹内，对每个文件进行循环遍历\n",
    "\n",
    "            file_type = item.rsplit('.')[1]\n",
    "            if file_type != 'mp3':\n",
    "                continue\n",
    "                \n",
    "            file_path = '{}/{}'.format(src_path, item)\n",
    "            print(file_path)\n",
    "            y, sr = librosa.load(file_path)\n",
    "            a = librosa.feature.mfcc(y=y, sr=sr)\n",
    "            x_data.append(get_max(a))\n",
    "            y_data.append(label_count)\n",
    "\n",
    "        label_count += 1\n",
    "\n",
    "    print('Train data size:', len(x_data))\n",
    "    print(len(y_data))\n",
    "    return x_data, y_data, label_count\n",
    "\n",
    "\n",
    "\n",
    "def write_train_file(input_file_dir, generate_file_path):\n",
    "    \"\"\"\n",
    "    生成训练文件\n",
    "    :param input_file_dir:\n",
    "    :param generate_file_path:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    x, y, label_count= generate_train_data(input_file_dir)\n",
    "\n",
    "    x, y = shuffer(x, y)\n",
    "    print('label_count: ', label_count)\n",
    "\n",
    "    f=open('{}/{}'.format(generate_file_path, 'x.txt'), 'w', encoding='UTF-8')\n",
    "    for line in x:\n",
    "        for a in line :\n",
    "            f.write(str(a)+',')\n",
    "        f.write('\\n')\n",
    "    f.close()\n",
    "\n",
    "    f = open('{}/{}'.format(generate_file_path, 'y.txt'), 'w', encoding='UTF-8')\n",
    "\n",
    "    for line in y:\n",
    "        a = int(line)\n",
    "        for i in range(label_count):\n",
    "            if i == a:\n",
    "                f.write('1')\n",
    "            else:\n",
    "                f.write('0')\n",
    "            if i < label_count-1:\n",
    "                f.write(',')\n",
    "\n",
    "        f.write('\\n')\n",
    "    f.close()\n",
    "    print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   提取特征\n",
    "\n",
    "声音文件的组织方式\n",
    "\n",
    "声音文件路径 ./test_data， 请改写\n",
    "\n",
    "```\n",
    "test-data\n",
    "    voice-class-1/\n",
    "        voice-1-01.mp3\n",
    "        voice-1-02.mp3\n",
    "        ...\n",
    "    voice-class-2/\n",
    "        ...\n",
    "    voice-class-3/\n",
    "        ...\n",
    "    ...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "target_dir = './target'\n",
    "if not os.path.exists(target_dir): \n",
    "    os.mkdir(target_dir)\n",
    "write_train_file('./test_data', target_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------\n",
    "### 第二步 编译word2vec\n",
    "用于生成词向量， 进行近义词比较， 将生成的可执行文件 word2vec copy到target文件夹里面\n",
    "\n",
    "```\n",
    "cd word2vec\n",
    "make\n",
    "cp word2vec ../target\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----------------------------------------------\n",
    "###  第三步 同义词训练\n",
    "\n",
    "生成词向量\n",
    "\n",
    "raw-data/english_train.txt  包含训练用的英语文章\n",
    "\n",
    "target/vec.txt 包含生成的词向量\n",
    "\n",
    "\n",
    "```shell\n",
    "\n",
    "cd shell\n",
    "./train.sh   '../raw-data/english_train.txt'   '../target/vec.txt'\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------\n",
    "###  第三步 创建查询同义词类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "\n",
    "class Synonym:\n",
    "    def __init__(self, word_vec_file_path, threshold_rate=0.6):\n",
    "        self.threshold_rate = threshold_rate\n",
    "        self.word_vec_file_path = word_vec_file_path\n",
    "        self.item_vec = self._load_item_vec(self.word_vec_file_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_item_vec(input_file):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_file: item vec file\n",
    "        Return:\n",
    "            dict key:itemid value:np.array([num1, num2....])\n",
    "        \"\"\"\n",
    "        if not os.path.exists(input_file):\n",
    "            return {}\n",
    "        linenum = 0\n",
    "        item_vec = {}\n",
    "        fp = open(input_file)\n",
    "        for line in fp:\n",
    "            if linenum == 0:\n",
    "                linenum += 1\n",
    "                continue\n",
    "            item = line.strip().split()\n",
    "            if len(item) < 129:\n",
    "                print(item)\n",
    "                continue\n",
    "            itemid = item[0]\n",
    "            if itemid == \"</s>\":\n",
    "                continue\n",
    "            item_vec[itemid] = np.array([float(ele) for ele in item[1:]])\n",
    "        fp.close()\n",
    "        return item_vec\n",
    "\n",
    "    def cal_item_sim(self, itemid):\n",
    "        \"\"\"\n",
    "        Args\n",
    "            item_vec:item embedding vector\n",
    "            itemid:fixed itemid to clac item sim\n",
    "            output_file: the file to store result\n",
    "        \"\"\"\n",
    "        if itemid not in self.item_vec:\n",
    "            return\n",
    "        score = {}\n",
    "        topk = 10\n",
    "        fix_item_vec = self.item_vec[itemid]\n",
    "        for tmp_itemid in self.item_vec:\n",
    "            if tmp_itemid == itemid:\n",
    "                continue\n",
    "            tmp_itemvec = self.item_vec[tmp_itemid]\n",
    "            fenmu = np.linalg.norm(fix_item_vec) * np.linalg.norm(tmp_itemvec)\n",
    "            if fenmu == 0:\n",
    "                score[tmp_itemid] = 0\n",
    "            else:\n",
    "                score[tmp_itemid] =  round(np.dot(fix_item_vec, tmp_itemvec)/fenmu, 3)\n",
    "        out_str = itemid + \"\\t\"\n",
    "        # print(out_str)\n",
    "        synonym_list = []\n",
    "        for zuhe in sorted(score.items(), key=operator.itemgetter(1), reverse=True)[:topk]:\n",
    "            if zuhe[1] > self.threshold_rate:\n",
    "                synonym_list.append(zuhe)\n",
    "                # print('{} :  {}'.format(zuhe[0], zuhe[1]))\n",
    "\n",
    "        return synonym_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonym = Synonym(\"./target/vec.txt\", 0.45)\n",
    "synonym.cal_item_sim('answer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------\n",
    "###  第五步 提取语法特征\n",
    "\n",
    "* 将语音通过AWS Transcribe 导出成文本\n",
    "* 通过AWS comprehend 分析名词 动词等关键语法信息\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "# 大于threshold_rate 算相似\n",
    "threshold_rate = 0.45\n",
    "\n",
    "class FeatureExtract:\n",
    "\n",
    "\n",
    "    def __init__(self, word_vec_file_path, right_content):\n",
    "        self.comprehend_client = boto3.client('comprehend')\n",
    "        self.word_type_name_list = ['NOUN', 'VERB']\n",
    "        self.score_dict = self.get_score_dict()\n",
    "        self.right_content = right_content\n",
    "        self.synonym = Synonym(\"./target/vec.txt\", threshold_rate)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_score_dict():\n",
    "        \"\"\"\n",
    "        测试数据， 后期可以从数据库读取\n",
    "        \"\"\"\n",
    "        score_dict = dict()\n",
    "        score_dict['184190001'] = 4.0\n",
    "        score_dict['184190003'] = 3.0\n",
    "        score_dict['184190010'] = 4.0\n",
    "        score_dict['184190020'] = 4.0\n",
    "        score_dict['184190045'] = 4.0\n",
    "\n",
    "        score_dict['184190058'] = 3.0\n",
    "        score_dict['184190071'] = 3.5\n",
    "        score_dict['184190081'] = 4.0\n",
    "        score_dict['184190109'] = 4.0\n",
    "        score_dict['184190151'] = 3.5\n",
    "\n",
    "        score_dict['184190170'] = 4.0\n",
    "        score_dict['184190177'] = 4.0\n",
    "        score_dict['184190189'] = 4.0\n",
    "        score_dict['184190199'] = 3.5\n",
    "        score_dict['184430141'] = 3.5\n",
    "        return score_dict\n",
    "\n",
    "    def read_json_file(self, file_path):\n",
    "        \"\"\"\n",
    "\n",
    "        :param file_path:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        with open(file_path, \"r\") as f:\n",
    "            new_dict = json.load(f)\n",
    "\n",
    "        return new_dict['results']['transcripts'][0]['transcript']\n",
    "\n",
    "    def create_word_dict(self, content):\n",
    "\n",
    "        result = self.comprehend_client.detect_syntax(Text= content, LanguageCode='en')\n",
    "        result = result['SyntaxTokens']\n",
    "        word_type_dict = dict()\n",
    "\n",
    "        for item in result:\n",
    "            tag_name = item['PartOfSpeech']['Tag']\n",
    "            if tag_name not in self.word_type_name_list:\n",
    "                continue\n",
    "\n",
    "            item_set = word_type_dict.get(tag_name)\n",
    "            if item_set is None:\n",
    "                item_set = set()\n",
    "\n",
    "            item_set.add(item['Text'].lower())\n",
    "            word_type_dict[tag_name] = item_set\n",
    "\n",
    "        for item in word_type_dict.items():\n",
    "            print('\\t', item)\n",
    "        return word_type_dict\n",
    "\n",
    "    def read_all_file(self, file):\n",
    "        \"\"\"\n",
    "        item（content, word_count ,word_dis_count,  word_type_dict ）\n",
    "        :param file:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        print(self.right_content)\n",
    "        word_dict = self.create_word_dict(self.right_content)\n",
    "        _word_dict_list = list()\n",
    "\n",
    "        count = 0\n",
    "        for root, dirs, files in os.walk(file):\n",
    "            for f in files:\n",
    "                if not f.endswith('json'):\n",
    "                    continue\n",
    "                print('\\n', os.path.join(root, f))\n",
    "                content = self.read_json_file(os.path.join(root, f))\n",
    "#                 if count >10:\n",
    "#                     continue\n",
    "#                 count +=1\n",
    "                word_count = len(content.split(' '))\n",
    "                word_dis_count = len(set(content.split(' ')))\n",
    "                print('word_count{}  word_dis_count {}'.format( word_count, word_dis_count))\n",
    "                word_type_dict = (f.split('.')[0], word_count, word_dis_count,  self.create_word_dict(content), content)\n",
    "                _word_dict_list.append(word_type_dict)\n",
    "        return _word_dict_list\n",
    "\n",
    "\n",
    "    def get_sim_score(self, base_list, new_list):\n",
    "        \"\"\"\n",
    "        获取相似度得分\n",
    "        :param base_list:\n",
    "        :param new_list:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        total_score = 0\n",
    "        for j in new_list:\n",
    "            if j in base_list:\n",
    "                total_score += 1.0\n",
    "            else:\n",
    "                synonym_list = self.synonym.cal_item_sim(j)\n",
    "                if synonym_list is None:\n",
    "                    continue\n",
    "                for syn_word in synonym_list:\n",
    "                    if syn_word[0] in base_list:\n",
    "                        total_score += float(syn_word[1])\n",
    "#                         print('word {} - > syn_word{}  score: {}'.format(j,  syn_word, total_score))\n",
    "                        break\n",
    "        return float('%.2f' % (total_score /len(base_list)))\n",
    "\n",
    "    def run(self):\n",
    "\n",
    "        word_dict_list = self.read_all_file('./raw-data')\n",
    "        count_index = 0\n",
    "        base_item = self.create_word_dict(self.right_content)\n",
    "        for word_type_name in self.word_type_name_list:\n",
    "            print('-------------- {}---------------- '.format(word_type_name))\n",
    "            tmp_item = sorted(list(base_item[word_type_name]))\n",
    "            score_dict = self.get_score_dict()\n",
    "\n",
    "            for item in word_dict_list:\n",
    "                words = sorted(list(item[3][word_type_name]))\n",
    "                sim_score = self.get_sim_score(tmp_item, words)\n",
    "\n",
    "                print('学号:{}\\t得分:{}\\t 单词个数:{}\\t不重复:{}\\t相似度:{}\\t {}个数: {}'.format(item[0], \n",
    "                                        score_dict[item[0]],  item[1], item[2], sim_score, word_type_name, len(words)))\n",
    "                count_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_right_content = \"\"\"It was a sunny day during last summer vacation. Ming practiced speaking English the whole morning. After that, he went to take piano lessons. Then his father took him to the art school to learn painting. Ming didn't have a rest until the evening. Unfortunately, he was so stressed out that he felt terrible. His parents sent him to hospital. And the doctor said Ming had a bad fever and should lie down and rest.\"\"\"\n",
    "featureExtract = FeatureExtract(\"./target/vec.txt\", _right_content)\n",
    "featureExtract.run()\n",
    "\n",
    "print('------------------------------ end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
